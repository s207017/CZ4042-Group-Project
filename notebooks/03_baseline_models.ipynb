{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Models\n",
        "\n",
        "This notebook trains and evaluates baseline models (LSTM, CNN, BiLSTM+Attention).\n",
        "\n",
        "## Objectives\n",
        "- Train baseline models on IMDB dataset\n",
        "- Compare model performance\n",
        "- Visualize training progress\n",
        "- Analyze errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from src.data.dataset_loader import load_preprocessed_data\n",
        "from src.data.preprocess import clean_text, create_vocabulary\n",
        "from src.models.lstm_model import LSTMModel, BiLSTMWithAttention\n",
        "from src.models.cnn_model import CNNModel\n",
        "from src.utils.seed_everything import seed_everything\n",
        "from src.utils.config_loader import load_config\n",
        "\n",
        "seed_everything(42)\n",
        "config = load_config('../config.yaml')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed IMDB data (from notebook 02) - already split into train/val/test\n",
        "from src.data.dataset_loader import load_preprocessed_data\n",
        "\n",
        "print(\"Loading preprocessed IMDB data (train/val/test splits)...\")\n",
        "train_texts, train_labels = load_preprocessed_data('imdb_train', data_dir='../intermediate/data')\n",
        "val_texts, val_labels = load_preprocessed_data('imdb_val', data_dir='../intermediate/data')\n",
        "test_texts, test_labels = load_preprocessed_data('imdb_test', data_dir='../intermediate/data')\n",
        "\n",
        "print(f\"âœ… Loaded preprocessed data (already split)\")\n",
        "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Vocabulary\n",
        "\n",
        "Note: Vocabulary creation uses the cleaned/preprocessed texts from the previous step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train LSTM Model\n",
        "\n",
        "Using preprocessed data - no need to clean again!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for baseline models (convert to sequences of word indices)\n",
        "# Note: This is a simplified version - full implementation would require tokenization\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize LSTM model\n",
        "lstm_model = LSTMModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=100,\n",
        "    hidden_dim=128,\n",
        "    num_layers=2,\n",
        "    num_classes=2,\n",
        "    dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "print(f\"LSTM Model parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create vocabulary for baseline models\n",
        "vocab = create_vocabulary(train_texts[:5000], min_freq=2)  # Use subset for speed\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
