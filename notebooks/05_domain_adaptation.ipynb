{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Domain Adaptation\n",
        "\n",
        "This notebook explores domain adaptation techniques for sentiment analysis.\n",
        "\n",
        "## Objectives\n",
        "- Train models on source domain (IMDB)\n",
        "- Adapt to target domain (Yelp)\n",
        "- Compare fine-tuning vs adversarial adaptation\n",
        "- Evaluate domain shift effects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from src.data.dataset_loader import load_preprocessed_data\n",
        "from src.models.transformer_model import BERTForSentiment\n",
        "from src.models.adapter_layers import DomainAdapter, AdversarialDiscriminator\n",
        "from src.train.train_transformer import train_transformer\n",
        "from src.utils.seed_everything import seed_everything\n",
        "from src.utils.config_loader import load_config\n",
        "\n",
        "seed_everything(42)\n",
        "config = load_config('../config.yaml')\n",
        "\n",
        "# Set device: MPS (Metal) for Apple Silicon, CUDA for NVIDIA, else CPU\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"✅ Using Apple Silicon GPU (MPS)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"✅ Using CUDA GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️ Using CPU (no GPU available)\")\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Preprocessed Source and Target Domains\n",
        "\n",
        "Load preprocessed data from notebook 02 - no need to clean again!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load preprocessed source domain (IMDB) - train split for domain adaptation\n",
        "print(\"Loading preprocessed source domain (IMDB train)...\")\n",
        "source_train_texts, source_train_labels = load_preprocessed_data('imdb_train', data_dir='../intermediate/data')\n",
        "source_val_texts, source_val_labels = load_preprocessed_data('imdb_val', data_dir='../intermediate/data')\n",
        "\n",
        "# Load preprocessed target domain (Yelp) - train split for fine-tuning\n",
        "print(\"Loading preprocessed target domain (Yelp train)...\")\n",
        "target_train_texts, target_train_labels = load_preprocessed_data('yelp_train', data_dir='../intermediate/data')\n",
        "target_val_texts, target_val_labels = load_preprocessed_data('yelp_val', data_dir='../intermediate/data')\n",
        "\n",
        "# Sample for domain adaptation experiments (use subset for faster training)\n",
        "target_train_texts = target_train_texts[:5000]\n",
        "target_train_labels = target_train_labels[:5000]\n",
        "target_val_texts = target_val_texts[:1000]\n",
        "target_val_labels = target_val_labels[:1000]\n",
        "\n",
        "print(f\"✅ Source (IMDB): Train={len(source_train_texts)}, Val={len(source_val_texts)}\")\n",
        "print(f\"✅ Target (Yelp): Train={len(target_train_texts)}, Val={len(target_val_texts)} (subset for faster training)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Train on Source Domain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders (data already split from preprocessing)\n",
        "from src.train.trainer_utils import create_dataloader\n",
        "\n",
        "# Use subset for faster training\n",
        "source_train_loader = create_dataloader(source_train_texts[:2000], source_train_labels[:2000],\n",
        "                                       'bert-base-uncased', batch_size=16, shuffle=True)\n",
        "source_val_loader = create_dataloader(source_val_texts[:500], source_val_labels[:500],\n",
        "                                     'bert-base-uncased', batch_size=16, shuffle=False)\n",
        "\n",
        "# Train on source domain\n",
        "source_model = BERTForSentiment('bert-base-uncased', num_classes=2).to(device)\n",
        "source_history = train_transformer(source_model, source_train_loader, source_val_loader, device,\n",
        "                                  num_epochs=2, learning_rate=2e-5)\n",
        "\n",
        "print(\"Source domain training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fine-tune on Target Domain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune on target domain (transfer learning) - data already split\n",
        "target_train_loader = create_dataloader(target_train_texts[:1000], target_train_labels[:1000],\n",
        "                                       'bert-base-uncased', batch_size=16, shuffle=True)\n",
        "target_val_loader = create_dataloader(target_val_texts[:200], target_val_labels[:200],\n",
        "                                     'bert-base-uncased', batch_size=16, shuffle=False)\n",
        "\n",
        "# Fine-tune the source model on target domain\n",
        "adapted_history = train_transformer(source_model, target_train_loader, target_val_loader, device,\n",
        "                                    num_epochs=2, learning_rate=1e-5)  # Lower LR for fine-tuning\n",
        "\n",
        "print(\"Domain adaptation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
