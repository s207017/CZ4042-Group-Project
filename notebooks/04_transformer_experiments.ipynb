{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer Experiments\n",
        "\n",
        "This notebook compares different Transformer architectures for sentiment analysis.\n",
        "\n",
        "## Objectives\n",
        "- Train BERT, RoBERTa, XLNet, and ELECTRA models\n",
        "- Compare performance across models\n",
        "- Analyze differences in learning curves\n",
        "- Evaluate on multiple datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from src.data.dataset_loader import load_preprocessed_data\n",
        "from src.models.transformer_model import BERTForSentiment, RoBERTaForSentiment, XLNetForSentiment, ELECTRAForSentiment\n",
        "from src.train.train_transformer import train_transformer, evaluate_transformer\n",
        "from src.train.trainer_utils import create_dataloader\n",
        "from src.evaluation.metrics import compute_metrics\n",
        "from src.evaluation.visualization import plot_training_history, plot_confusion_matrix\n",
        "from src.utils.seed_everything import seed_everything\n",
        "from src.utils.config_loader import load_config\n",
        "\n",
        "seed_everything(42)\n",
        "config = load_config('../config.yaml')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed IMDB data (from notebook 02) - already split into train/val/test\n",
        "print(\"Loading preprocessed IMDB data (train/val/test splits)...\")\n",
        "train_texts, train_labels = load_preprocessed_data('imdb_train', data_dir='../intermediate/data')\n",
        "val_texts, val_labels = load_preprocessed_data('imdb_val', data_dir='../intermediate/data')\n",
        "test_texts, test_labels = load_preprocessed_data('imdb_test', data_dir='../intermediate/data')\n",
        "\n",
        "print(f\"âœ… Loaded preprocessed data (already split)\")\n",
        "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train BERT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "train_loader = create_dataloader(train_texts[:1000], train_labels[:1000], \n",
        "                                'bert-base-uncased', batch_size=16, shuffle=True)\n",
        "val_loader = create_dataloader(val_texts[:200], val_labels[:200],\n",
        "                               'bert-base-uncased', batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize BERT model\n",
        "model = BERTForSentiment('bert-base-uncased', num_classes=2).to(device)\n",
        "\n",
        "# Train model\n",
        "history = train_transformer(model, train_loader, val_loader, device,\n",
        "                           num_epochs=3, learning_rate=2e-5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualize Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "test_loader = create_dataloader(test_texts[:500], test_labels[:500],\n",
        "                                'bert-base-uncased', batch_size=16, shuffle=False)\n",
        "\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_metrics = evaluate_transformer(model, test_loader, device, criterion)\n",
        "\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
        "print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"Test F1: {test_metrics['f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plot_training_history(history, save_path='../experiments/plots/bert_training.png')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
