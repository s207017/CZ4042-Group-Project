{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing Pipeline\n",
        "\n",
        "This notebook demonstrates the preprocessing pipeline for sentiment analysis.\n",
        "\n",
        "## Objectives\n",
        "- Test text cleaning functions\n",
        "- Compare tokenization methods\n",
        "- Analyze preprocessing effects\n",
        "- Create vocabulary for baseline models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/seunghwanchoi/miniforge3/envs/sentiment-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Seed set to 42 for reproducible data splitting\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from src.data.preprocess import clean_text, tokenize_texts, create_vocabulary\n",
        "from src.data.dataset_loader import IMDBDataLoader, YelpDataLoader, SST2Loader, create_train_test_split\n",
        "from src.utils.seed_everything import seed_everything\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_everything(42)\n",
        "print(\"✅ Seed set to 42 for reproducible data splitting\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Text Cleaning\n",
        "\n",
        "### Preprocessing Steps:\n",
        "\n",
        "The `clean_text()` function performs the following operations:\n",
        "\n",
        "1. **Remove HTML Tags**: Removes all HTML/XML tags (e.g., `<br>`, `<div>`, `<p>`)\n",
        "   - Pattern: `r'<[^>]+>'`\n",
        "   - Example: `\"Hello<br>World\"` → `\"HelloWorld\"`\n",
        "\n",
        "2. **Remove URLs**: Removes web URLs (http/https and www links)\n",
        "   - Pattern: `r'http\\S+|www\\.\\S+'`\n",
        "   - Example: `\"Check http://example.com\"` → `\"Check \"`\n",
        "\n",
        "3. **Remove Special Characters**: Removes special characters but **keeps**:\n",
        "   - Alphanumeric characters (a-z, A-Z, 0-9)\n",
        "   - Basic punctuation: `. , ! ?`\n",
        "   - Whitespace\n",
        "   - Pattern: `r'[^\\w\\s.,!?]'`\n",
        "   - Example: `\"@user #tag $price\"` → `\"user tag price\"`\n",
        "\n",
        "4. **Normalize Whitespace**: Replaces multiple consecutive spaces with a single space\n",
        "   - Pattern: `r'\\s+'`\n",
        "   - Example: `\"Hello    World\"` → `\"Hello World\"`\n",
        "\n",
        "5. **Strip**: Removes leading and trailing whitespace\n",
        "\n",
        "### What Gets Removed:\n",
        "- ❌ HTML/XML tags (`<br>`, `<div>`, etc.)\n",
        "- ❌ URLs (`http://...`, `www....`)\n",
        "- ❌ Special symbols (`@`, `#`, `$`, `%`, `&`, `*`, `()`, `[]`, `{}`, etc.)\n",
        "- ❌ Extra whitespace (multiple spaces/tabs/newlines)\n",
        "\n",
        "### What Gets Kept:\n",
        "- ✅ Alphanumeric characters\n",
        "- ✅ Basic punctuation (period, comma, exclamation, question mark)\n",
        "- ✅ Words and numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "This movie was <br>AWESOME!!! http://example.com  Check it out @movie #excited\n",
            "\n",
            "Cleaned text:\n",
            "This movie was AWESOME!!! Check it out movie excited\n"
          ]
        }
      ],
      "source": [
        "# Sample dirty text\n",
        "dirty_text = \"This movie was <br>AWESOME!!! http://example.com  Check it out @movie #excited\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(dirty_text)\n",
        "print(\"\\nCleaned text:\")\n",
        "print(clean_text(dirty_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Examples:\n",
            "\n",
            "Original: This movie <br>is great! http://example.com @movie #excited\n",
            "→ Original This movie is great! movie excited\n",
            "Matches expected: False\n",
            "\n",
            "Original: AWESOME!!!  Multiple    spaces   here.\n",
            "→ Original AWESOME!!! Multiple spaces here.\n",
            "Matches expected: False\n",
            "\n",
            "Original: Check $price & quality @store #sale\n",
            "→ Original Check price quality store sale\n",
            "Matches expected: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show detailed examples of what gets removed\n",
        "examples = [\n",
        "    (\"Original: This movie <br>is great! http://example.com @movie #excited\", \n",
        "     \"Cleaned: This movie is great! movie excited\"),\n",
        "    (\"Original: AWESOME!!!  Multiple    spaces   here.\",\n",
        "     \"Cleaned: AWESOME!!! Multiple spaces here.\"),\n",
        "    (\"Original: Check $price & quality @store #sale\",\n",
        "     \"Cleaned: Check price quality store sale\"),\n",
        "]\n",
        "\n",
        "print(\"Preprocessing Examples:\\n\")\n",
        "for original, expected in examples:\n",
        "    cleaned = clean_text(original)\n",
        "    print(f\"{original}\")\n",
        "    print(f\"→ {cleaned}\")\n",
        "    print(f\"Matches expected: {cleaned == expected}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 IMDB Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMDB Dataset:\n",
            "Original avg length: 231.3 words\n",
            "Cleaned avg length: 225.9 words\n",
            "Length reduction: 5.4 words\n"
          ]
        }
      ],
      "source": [
        "# Load and clean IMDB data\n",
        "imdb_loader = IMDBDataLoader('../IMDB Dataset.csv')\n",
        "imdb_texts, imdb_labels = imdb_loader.load(binary=True)\n",
        "\n",
        "# Clean a sample\n",
        "imdb_sample = imdb_texts[:100]\n",
        "imdb_cleaned = [clean_text(text) for text in imdb_sample]\n",
        "\n",
        "print(f\"IMDB Dataset:\")\n",
        "print(f\"Original avg length: {np.mean([len(t.split()) for t in imdb_sample]):.1f} words\")\n",
        "print(f\"Cleaned avg length: {np.mean([len(t.split()) for t in imdb_cleaned]):.1f} words\")\n",
        "print(f\"Length reduction: {np.mean([len(t.split()) for t in imdb_sample]) - np.mean([len(t.split()) for t in imdb_cleaned]):.1f} words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 SST-2 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SST-2 Dataset:\n",
            "Original avg length: 19.9 words\n",
            "Cleaned avg length: 19.6 words\n",
            "Length reduction: 0.3 words\n"
          ]
        }
      ],
      "source": [
        "# Load and clean SST-2 data\n",
        "sst2_path = Path('../archive (5)/SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank')\n",
        "sst2_loader = SST2Loader(sst2_path)\n",
        "sst2_train_texts, sst2_train_labels, _, _, _, _ = sst2_loader.load()\n",
        "\n",
        "# Clean a sample\n",
        "sst2_sample = sst2_train_texts[:500]  # SST-2 sentences are shorter\n",
        "sst2_cleaned = [clean_text(text) for text in sst2_sample]\n",
        "\n",
        "print(f\"SST-2 Dataset:\")\n",
        "print(f\"Original avg length: {np.mean([len(t.split()) for t in sst2_sample]):.1f} words\")\n",
        "print(f\"Cleaned avg length: {np.mean([len(t.split()) for t in sst2_cleaned]):.1f} words\")\n",
        "print(f\"Length reduction: {np.mean([len(t.split()) for t in sst2_sample]) - np.mean([len(t.split()) for t in sst2_cleaned]):.1f} words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Yelp Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yelp Dataset:\n",
            "Original avg length: 88.8 words\n",
            "Cleaned avg length: 88.6 words\n",
            "Length reduction: 0.3 words\n"
          ]
        }
      ],
      "source": [
        "# Load and clean Yelp data\n",
        "yelp_loader = YelpDataLoader('../archive (7)/yelp_academic_dataset_review.json')\n",
        "yelp_texts, yelp_labels = yelp_loader.load(sample_size=5000, binary=True)\n",
        "\n",
        "# Clean a sample\n",
        "yelp_sample = yelp_texts[:100]\n",
        "yelp_cleaned = [clean_text(text) for text in yelp_sample]\n",
        "\n",
        "print(f\"Yelp Dataset:\")\n",
        "print(f\"Original avg length: {np.mean([len(t.split()) for t in yelp_sample]):.1f} words\")\n",
        "print(f\"Cleaned avg length: {np.mean([len(t.split()) for t in yelp_cleaned]):.1f} words\")\n",
        "print(f\"Length reduction: {np.mean([len(t.split()) for t in yelp_sample]) - np.mean([len(t.split()) for t in yelp_cleaned]):.1f} words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Preprocessing Comparison Across Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Effects Comparison:\n",
            "\n",
            "Dataset  Original Avg Length  Cleaned Avg Length  Reduction  Reduction %\n",
            "   IMDB               231.31              225.86       5.45         2.36\n",
            "  SST-2                21.43               20.70       0.73         3.41\n",
            "   Yelp                88.85               88.57       0.28         0.32\n"
          ]
        }
      ],
      "source": [
        "# Compare preprocessing effects across datasets\n",
        "comparison_data = {\n",
        "    'Dataset': ['IMDB', 'SST-2', 'Yelp'],\n",
        "    'Original Avg Length': [\n",
        "        np.mean([len(t.split()) for t in imdb_sample]),\n",
        "        np.mean([len(t.split()) for t in sst2_sample[:100]]),\n",
        "        np.mean([len(t.split()) for t in yelp_sample])\n",
        "    ],\n",
        "    'Cleaned Avg Length': [\n",
        "        np.mean([len(t.split()) for t in imdb_cleaned]),\n",
        "        np.mean([len(t.split()) for t in sst2_cleaned[:100]]),\n",
        "        np.mean([len(t.split()) for t in yelp_cleaned])\n",
        "    ],\n",
        "    'Reduction': [\n",
        "        np.mean([len(t.split()) for t in imdb_sample]) - np.mean([len(t.split()) for t in imdb_cleaned]),\n",
        "        np.mean([len(t.split()) for t in sst2_sample[:100]]) - np.mean([len(t.split()) for t in sst2_cleaned[:100]]),\n",
        "        np.mean([len(t.split()) for t in yelp_sample]) - np.mean([len(t.split()) for t in yelp_cleaned])\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Reduction %'] = (comparison_df['Reduction'] / comparison_df['Original Avg Length'] * 100).round(2)\n",
        "\n",
        "print(\"Preprocessing Effects Comparison:\\n\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample preprocessing examples:\n",
            "\n",
            "\n",
            "IMDB:\n",
            "  Original: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The...\n",
            "  Cleaned:  One of the other reviewers has mentioned that after watching just 1 Oz episode youll be hooked. They...\n",
            "\n",
            "SST-2:\n",
            "  Original: The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash...\n",
            "  Cleaned:  The Rock is destined to be the 21st Century s new Conan and that he s going to make a splash even gr...\n",
            "\n",
            "Yelp:\n",
            "  Original: If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We...\n",
            "  Cleaned:  If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We...\n"
          ]
        }
      ],
      "source": [
        "# Show sample before/after for each dataset\n",
        "print(\"Sample preprocessing examples:\\n\")\n",
        "\n",
        "datasets = [\n",
        "    (\"IMDB\", imdb_sample[0]),\n",
        "    (\"SST-2\", sst2_sample[0]),\n",
        "    (\"Yelp\", yelp_sample[0])\n",
        "]\n",
        "\n",
        "for name, original in datasets:\n",
        "    cleaned = clean_text(original)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Original: {original[:100]}...\")\n",
        "    print(f\"  Cleaned:  {cleaned[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Preprocessed Data\n",
        "\n",
        "Save preprocessed datasets for use in subsequent notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning IMDB dataset...\n",
            "Splitting IMDB into train/val/test sets...\n",
            "Saved IMDB: Train=35000, Val=5000, Test=10000\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessed IMDB data (with train/val/test split)\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "intermediate_dir = Path('../intermediate/data')\n",
        "intermediate_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load full IMDB dataset\n",
        "imdb_loader = IMDBDataLoader('../IMDB Dataset.csv')\n",
        "imdb_texts, imdb_labels = imdb_loader.load(binary=True)\n",
        "\n",
        "# Clean all texts\n",
        "print(\"Cleaning IMDB dataset...\")\n",
        "imdb_cleaned_full = [clean_text(text) for text in imdb_texts]\n",
        "\n",
        "# Split into train/val/test (using seed from seed_everything)\n",
        "print(\"Splitting IMDB into train/val/test sets...\")\n",
        "imdb_train_texts, imdb_val_texts, imdb_test_texts, \\\n",
        "imdb_train_labels, imdb_val_labels, imdb_test_labels = create_train_test_split(\n",
        "    imdb_cleaned_full, imdb_labels, test_size=0.2, val_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Save as separate CSV files\n",
        "imdb_train_df = pd.DataFrame({'text': imdb_train_texts, 'label': imdb_train_labels})\n",
        "imdb_val_df = pd.DataFrame({'text': imdb_val_texts, 'label': imdb_val_labels})\n",
        "imdb_test_df = pd.DataFrame({'text': imdb_test_texts, 'label': imdb_test_labels})\n",
        "\n",
        "imdb_train_df.to_csv(intermediate_dir / 'imdb_train_preprocessed.csv', index=False)\n",
        "imdb_val_df.to_csv(intermediate_dir / 'imdb_val_preprocessed.csv', index=False)\n",
        "imdb_test_df.to_csv(intermediate_dir / 'imdb_test_preprocessed.csv', index=False)\n",
        "\n",
        "print(f\"Saved IMDB: Train={len(imdb_train_texts)}, Val={len(imdb_val_texts)}, Test={len(imdb_test_texts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning SST-2 dataset...\n",
            "Saved SST-2: Train=8544, Val=1101, Test=2210\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessed SST-2 data\n",
        "sst2_path = Path('../archive (5)/SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank')\n",
        "sst2_loader = SST2Loader(sst2_path)\n",
        "sst2_train_texts, sst2_train_labels, sst2_val_texts, sst2_val_labels, sst2_test_texts, sst2_test_labels = sst2_loader.load()\n",
        "\n",
        "# Clean all texts\n",
        "print(\"Cleaning SST-2 dataset...\")\n",
        "sst2_train_cleaned = [clean_text(text) for text in sst2_train_texts]\n",
        "sst2_val_cleaned = [clean_text(text) for text in sst2_val_texts]\n",
        "sst2_test_cleaned = [clean_text(text) for text in sst2_test_texts]\n",
        "\n",
        "# Save as CSV\n",
        "train_df = pd.DataFrame({'text': sst2_train_cleaned, 'label': sst2_train_labels})\n",
        "val_df = pd.DataFrame({'text': sst2_val_cleaned, 'label': sst2_val_labels})\n",
        "test_df = pd.DataFrame({'text': sst2_test_cleaned, 'label': sst2_test_labels})\n",
        "\n",
        "train_df.to_csv(intermediate_dir / 'sst2_train_preprocessed.csv', index=False)\n",
        "val_df.to_csv(intermediate_dir / 'sst2_val_preprocessed.csv', index=False)\n",
        "test_df.to_csv(intermediate_dir / 'sst2_test_preprocessed.csv', index=False)\n",
        "\n",
        "print(f\"Saved SST-2: Train={len(sst2_train_cleaned)}, Val={len(sst2_val_cleaned)}, Test={len(sst2_test_cleaned)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning Yelp dataset...\n",
            "Splitting Yelp into train/val/test sets...\n",
            "Saved Yelp: Train=35000, Val=5000, Test=10000\n",
            "\n",
            "✅ All preprocessed data saved to ../intermediate/data\n"
          ]
        }
      ],
      "source": [
        "# Save preprocessed Yelp data (with train/val/test split)\n",
        "yelp_loader = YelpDataLoader('../archive (7)/yelp_academic_dataset_review.json')\n",
        "yelp_texts, yelp_labels = yelp_loader.load(sample_size=50000, binary=True)\n",
        "\n",
        "# Clean all texts\n",
        "print(\"Cleaning Yelp dataset...\")\n",
        "yelp_cleaned_full = [clean_text(text) for text in yelp_texts]\n",
        "\n",
        "# Split into train/val/test (using seed from seed_everything)\n",
        "print(\"Splitting Yelp into train/val/test sets...\")\n",
        "yelp_train_texts, yelp_val_texts, yelp_test_texts, \\\n",
        "yelp_train_labels, yelp_val_labels, yelp_test_labels = create_train_test_split(\n",
        "    yelp_cleaned_full, yelp_labels, test_size=0.2, val_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Save as separate CSV files\n",
        "yelp_train_df = pd.DataFrame({'text': yelp_train_texts, 'label': yelp_train_labels})\n",
        "yelp_val_df = pd.DataFrame({'text': yelp_val_texts, 'label': yelp_val_labels})\n",
        "yelp_test_df = pd.DataFrame({'text': yelp_test_texts, 'label': yelp_test_labels})\n",
        "\n",
        "yelp_train_df.to_csv(intermediate_dir / 'yelp_train_preprocessed.csv', index=False)\n",
        "yelp_val_df.to_csv(intermediate_dir / 'yelp_val_preprocessed.csv', index=False)\n",
        "yelp_test_df.to_csv(intermediate_dir / 'yelp_test_preprocessed.csv', index=False)\n",
        "\n",
        "print(f\"Saved Yelp: Train={len(yelp_train_texts)}, Val={len(yelp_val_texts)}, Test={len(yelp_test_texts)}\")\n",
        "\n",
        "print(f\"\\n✅ All preprocessed data saved to {intermediate_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sentiment-analysis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
